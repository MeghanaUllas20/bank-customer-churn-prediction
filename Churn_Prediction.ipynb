{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO2qgq5Sryya"
      },
      "outputs": [],
      "source": [
        "# Install PySpark\n",
        "!pip install -q pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvWESBiLsBdJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BankingCustomerChurn\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhD8AnnNsGdV"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # click \"Choose Files\" and upload your CSV, e.g., Churn_Modelling.csv\n",
        "\n",
        "# After upload, list files to confirm\n",
        "import os\n",
        "print(\"Files in /content/:\")\n",
        "print(os.listdir(\"/content\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za0E9K8QsP2G"
      },
      "outputs": [],
      "source": [
        "# Change filename if different\n",
        "csv_path = \"/content/Churn_Modelling.csv\"\n",
        "\n",
        "df = spark.read.option(\"header\", \"true\") \\\n",
        "               .option(\"inferSchema\", \"true\") \\\n",
        "               .csv(csv_path)\n",
        "\n",
        "# Basic checks\n",
        "print(\"Number of rows:\", df.count())\n",
        "print(\"Columns:\", df.columns)\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ka-3bppsdTx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Drop columns that are identifiers or not useful\n",
        "cols_to_drop = [\"RowNumber\", \"CustomerId\", \"Surname\"]\n",
        "df_clean = df.drop(*cols_to_drop)\n",
        "\n",
        "# Verify\n",
        "print(\"Columns after dropping:\", df_clean.columns)\n",
        "df_clean.groupBy(\"Exited\").count().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS9kf8mUsilm"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "\n",
        "categorical_cols = [\"Geography\", \"Gender\"]\n",
        "numeric_cols = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\",\n",
        "                \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"]\n",
        "\n",
        "label_col = \"Exited\"\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=c, outputCol=c + \"_index\", handleInvalid=\"keep\")\n",
        "    for c in categorical_cols\n",
        "]\n",
        "\n",
        "encoders = [\n",
        "    OneHotEncoder(inputCols=[c + \"_index\"], outputCols=[c + \"_ohe\"])\n",
        "    for c in categorical_cols\n",
        "]\n",
        "\n",
        "assembler_inputs = numeric_cols + [c + \"_ohe\" for c in categorical_cols]\n",
        "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCXVAQ7Csl2k"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"Train rows:\", train_df.count())\n",
        "print(\"Test rows:\", test_df.count())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIy7OctHsqZz"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col, maxIter=20)\n",
        "\n",
        "pipeline_lr = Pipeline(stages=indexers + encoders + [assembler, lr])\n",
        "lr_model = pipeline_lr.fit(train_df)\n",
        "\n",
        "lr_predictions = lr_model.transform(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTp2Rof6suli"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=label_col, metricName=\"areaUnderROC\")\n",
        "\n",
        "lr_auc = evaluator.evaluate(lr_predictions)\n",
        "print(\"Logistic Regression AUC:\", lr_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbML1bMEs6E3"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=label_col,\n",
        "                            numTrees=100, maxDepth=6, seed=42)\n",
        "\n",
        "pipeline_rf = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
        "rf_model = pipeline_rf.fit(train_df)\n",
        "\n",
        "rf_predictions = rf_model.transform(test_df)\n",
        "\n",
        "rf_auc = evaluator.evaluate(rf_predictions)\n",
        "print(\"Random Forest AUC:\", rf_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJsKyZmXs9CN"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(featuresCol=\"features\", labelCol=label_col,\n",
        "                    maxIter=50, maxDepth=5, stepSize=0.1, seed=42)\n",
        "\n",
        "pipeline_gbt = Pipeline(stages=indexers + encoders + [assembler, gbt])\n",
        "gbt_model = pipeline_gbt.fit(train_df)\n",
        "\n",
        "gbt_predictions = gbt_model.transform(test_df)\n",
        "\n",
        "gbt_auc = evaluator.evaluate(gbt_predictions)\n",
        "print(\"GBT AUC:\", gbt_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBfO6l8vtCNi"
      },
      "outputs": [],
      "source": [
        "print(\"Model Comparison:\")\n",
        "print(\"LR AUC:\", lr_auc)\n",
        "print(\"RF AUC:\", rf_auc)\n",
        "print(\"GBT AUC:\", gbt_auc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ-LeTbptFjm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gbt_pd = gbt_predictions.select(\"prediction\", \"Exited\").toPandas()\n",
        "\n",
        "y_true = gbt_pd[\"Exited\"]\n",
        "y_pred = gbt_pd[\"prediction\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"GBT Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ESW3bQVtK07"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "gbt_pd = gbt_predictions.select(\"rawPrediction\", \"Exited\").toPandas()\n",
        "gbt_pd[\"score\"] = gbt_pd[\"rawPrediction\"].apply(lambda x: float(x[1]))\n",
        "\n",
        "y_true = gbt_pd[\"Exited\"]\n",
        "y_scores = gbt_pd[\"score\"]\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.plot(fpr, tpr, label=f'GBT ROC (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"GBT ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlFkQp3FtNxP"
      },
      "outputs": [],
      "source": [
        "rf_stage_model = rf_model.stages[-1]\n",
        "importances = rf_stage_model.featureImportances\n",
        "\n",
        "feature_importance = list(zip(assembler_inputs, importances.toArray()))\n",
        "feature_importance_sorted = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, score in feature_importance_sorted:\n",
        "    print(f\"{name}: {score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this project, multiple machine learning models were trained using PySpark to predict bank customer churn.\n",
        "\n",
        "Among all models, Gradient Boosted Trees (GBT) achieved the highest AUC score, making it the best-performing model.\n",
        "\n",
        "The model can help banks identify customers who are likely to churn and take proactive steps to improve retention.\n"
      ],
      "metadata": {
        "id": "kmVKEJH6I9wG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}