{"cells":[{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":7840,"status":"ok","timestamp":1771336759900,"user":{"displayName":"Meghana U","userId":"01018691188298802427"},"user_tz":-330},"id":"YO2qgq5Sryya"},"outputs":[],"source":["# Install PySpark (only once)\n","!pip install -q pyspark\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1771336759930,"user":{"displayName":"Meghana U","userId":"01018691188298802427"},"user_tz":-330},"id":"vvWESBiLsBdJ","outputId":"0db0a5c2-ffa9-482a-90d6-75ec35397a04"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7d70e93b7620>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://d02512ecae6d:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v4.0.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>BankingCustomerChurn</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":26}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"BankingCustomerChurn\") \\\n","    .getOrCreate()\n","\n","spark\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"GhD8AnnNsGdV","outputId":"165e0da4-d5e7-494e-968d-1b0d4f3ac286"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-19c356d6-2f17-42ed-8e6b-70089b4ee24f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-19c356d6-2f17-42ed-8e6b-70089b4ee24f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}}],"source":["from google.colab import files\n","uploaded = files.upload()  # click \"Choose Files\" and upload your CSV, e.g., Churn_Modelling.csv\n","\n","# After upload, list files to confirm\n","import os\n","print(\"Files in /content/:\")\n","print(os.listdir(\"/content\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"za0E9K8QsP2G"},"outputs":[],"source":["# Change filename if different\n","csv_path = \"/content/Churn_Modelling.csv\"\n","\n","df = spark.read.option(\"header\", \"true\") \\\n","               .option(\"inferSchema\", \"true\") \\\n","               .csv(csv_path)\n","\n","# Basic checks\n","print(\"Number of rows:\", df.count())\n","print(\"Columns:\", df.columns)\n","df.printSchema()\n","df.show(5, truncate=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ka-3bppsdTx"},"outputs":[],"source":["from pyspark.sql.functions import col\n","\n","# Drop columns that are identifiers or not useful\n","cols_to_drop = [\"RowNumber\", \"CustomerId\", \"Surname\"]\n","df_clean = df.drop(*cols_to_drop)\n","\n","# Verify\n","print(\"Columns after dropping:\", df_clean.columns)\n","df_clean.groupBy(\"Exited\").count().show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS9kf8mUsilm"},"outputs":[],"source":["# Convert cleaned DataFrame to RDD for RDD-style ops\n","rdd = df_clean.rdd\n","\n","# 1. Map + reduceByKey: count customers per Geography\n","geo_rdd = rdd.map(lambda row: (row.Geography, 1))\n","geo_counts = geo_rdd.reduceByKey(lambda a, b: a + b).collect()\n","print(\"Customer count by Geography (RDD):\", geo_counts)\n","\n","# 2. Filter: active members count\n","active_count = rdd.filter(lambda row: row.IsActiveMember == 1).count()\n","print(\"Active Members (RDD):\", active_count)\n","\n","# 3. Churn distribution\n","churn_dist = rdd.map(lambda row: (\"Churned\" if row.Exited == 1 else \"Not Churned\", 1)) \\\n","                .reduceByKey(lambda a, b: a + b).collect()\n","print(\"Churn Distribution (RDD):\", churn_dist)\n","\n","# 4. High balance customers (RDD)\n","high_balance_rdd = rdd.filter(lambda row: (row.Balance is not None) and (row.Balance > 100000))\n","print(\"Customers with balance > 100000 (RDD):\", high_balance_rdd.count())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCXVAQ7Csl2k"},"outputs":[],"source":["# Group by Geography\n","print(\"Customer count by Geography (DataFrame):\")\n","df_clean.groupBy(\"Geography\").count().show()\n","\n","# Average balance by Exited\n","print(\"Average balance by churn status:\")\n","df_clean.groupBy(\"Exited\").avg(\"Balance\").show()\n","\n","# Average balance by Gender\n","print(\"Average balance by gender:\")\n","df_clean.groupBy(\"Gender\").agg({\"Balance\": \"avg\"}).show()\n","\n","# Top 5 by CreditScore\n","print(\"Top 5 highest credit score customers:\")\n","df_clean.orderBy(df_clean.CreditScore.desc()).show(5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PIy7OctHsqZz"},"outputs":[],"source":["from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n","from pyspark.ml import Pipeline\n","\n","# Columns (kept same as your code)\n","categorical_cols = [\"Geography\", \"Gender\"]\n","numeric_cols = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\",\n","                \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"]\n","label_col = \"Exited\"\n","\n","# Indexers (handleInvalid=\"keep\" to be safe)\n","indexers = [\n","    StringIndexer(inputCol=col_name, outputCol=col_name + \"_index\", handleInvalid=\"keep\")\n","    for col_name in categorical_cols\n","]\n","\n","# OneHotEncoders\n","encoders = [\n","    OneHotEncoder(inputCols=[col_name + \"_index\"], outputCols=[col_name + \"_ohe\"])\n","    for col_name in categorical_cols\n","]\n","\n","# Assembler\n","assembler_inputs = numeric_cols + [c + \"_ohe\" for c in categorical_cols]\n","assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n","\n","print(\"Assembler inputs:\", assembler_inputs)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTp2Rof6suli"},"outputs":[],"source":["train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n","print(\"Train rows:\", train_df.count())\n","print(\"Test rows:\", test_df.count())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbML1bMEs6E3"},"outputs":[],"source":["from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml import Pipeline\n","\n","lr = LogisticRegression(featuresCol=\"features\", labelCol=label_col, maxIter=20)\n","\n","stages = []\n","stages.extend(indexers)\n","stages.extend(encoders)\n","stages.append(assembler)\n","stages.append(lr)\n","\n","pipeline = Pipeline(stages=stages)\n","lr_model = pipeline.fit(train_df)\n","\n","print(\"Logistic Regression training done ✅\")\n","\n","predictions = lr_model.transform(test_df)\n","predictions.select(\"features\", \"Exited\", \"probability\", \"prediction\").show(5, truncate=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJsKyZmXs9CN"},"outputs":[],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.sql.functions import when\n","\n","# AUC\n","evaluator_auc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n","auc = evaluator_auc.evaluate(predictions)\n","print(\"LR AUC:\", auc)\n","\n","# Accuracy (simple compute)\n","predictions = predictions.withColumn(\"correct\", when(predictions[label_col] == predictions[\"prediction\"], 1).otherwise(0))\n","accuracy = predictions.agg({\"correct\": \"avg\"}).collect()[0][0]\n","print(\"LR Accuracy:\", accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBfO6l8vtCNi"},"outputs":[],"source":["from pyspark.ml.classification import RandomForestClassifier\n","\n","rf = RandomForestClassifier(featuresCol=\"features\", labelCol=label_col, numTrees=100, maxDepth=6, seed=42)\n","\n","rf_stages = []\n","rf_stages.extend(indexers)\n","rf_stages.extend(encoders)\n","rf_stages.append(assembler)\n","rf_stages.append(rf)\n","\n","rf_pipeline = Pipeline(stages=rf_stages)\n","rf_model = rf_pipeline.fit(train_df)\n","\n","print(\"Random Forest model training done ✅\")\n","\n","rf_predictions = rf_model.transform(test_df)\n","rf_predictions.select(\"Exited\", \"probability\", \"prediction\").show(5, truncate=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJ-LeTbptFjm"},"outputs":[],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.sql.functions import when\n","\n","rf_evaluator_auc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n","rf_auc = rf_evaluator_auc.evaluate(rf_predictions)\n","print(\"RF AUC:\", rf_auc)\n","\n","rf_predictions = rf_predictions.withColumn(\"correct\", when(rf_predictions[label_col] == rf_predictions[\"prediction\"], 1).otherwise(0))\n","rf_accuracy = rf_predictions.agg({\"correct\": \"avg\"}).collect()[0][0]\n","print(\"RF Accuracy:\", rf_accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ESW3bQVtK07"},"outputs":[],"source":["# Get RandomForestClassificationModel (last stage)\n","rf_stage_model = rf_model.stages[-1]\n","importances = rf_stage_model.featureImportances\n","\n","# Feature names in same order as assembler_inputs\n","feature_names = assembler_inputs\n","\n","feature_importance = list(zip(feature_names, importances.toArray()))\n","feature_importance_sorted = sorted(feature_importance, key=lambda x: x[1], reverse=True)\n","\n","print(\"Feature Importances (sorted):\")\n","for name, score in feature_importance_sorted:\n","    print(f\"{name}: {score}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlFkQp3FtNxP"},"outputs":[],"source":["from pyspark.ml.classification import GBTClassifier\n","\n","gbt = GBTClassifier(featuresCol=\"features\", labelCol=label_col, maxIter=50, maxDepth=5, stepSize=0.1, seed=42)\n","\n","gbt_stages = []\n","gbt_stages.extend(indexers)\n","gbt_stages.extend(encoders)\n","gbt_stages.append(assembler)\n","gbt_stages.append(gbt)\n","\n","gbt_pipeline = Pipeline(stages=gbt_stages)\n","gbt_model = gbt_pipeline.fit(train_df)\n","\n","print(\"GBT model training done ✅\")\n","\n","gbt_predictions = gbt_model.transform(test_df)\n","gbt_predictions.select(\"Exited\", \"probability\", \"prediction\").show(5, truncate=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_dkldiItQS7"},"outputs":[],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.sql.functions import when\n","\n","gbt_evaluator_auc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n","gbt_auc = gbt_evaluator_auc.evaluate(gbt_predictions)\n","print(\"GBT AUC:\", gbt_auc)\n","\n","gbt_predictions = gbt_predictions.withColumn(\"correct\", when(gbt_predictions[label_col] == gbt_predictions[\"prediction\"], 1).otherwise(0))\n","gbt_accuracy = gbt_predictions.agg({\"correct\": \"avg\"}).collect()[0][0]\n","print(\"GBT Accuracy:\", gbt_accuracy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fADcb2lrtbFc"},"outputs":[],"source":["# Save GBT predictions as parquet\n","gbt_predictions.write.mode(\"overwrite\").parquet(\"/content/churn_predictions_parquet\")\n","print(\"Predictions saved successfully at /content/churn_predictions_parquet\")\n","\n","# Save the trained GBT pipeline model\n","gbt_model.write().overwrite().save(\"/content/gbt_churn_model\")\n","print(\"GBT Model saved successfully at /content/gbt_churn_model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0xXk2FyteYE"},"outputs":[],"source":["import pandas as pd\n","\n","# Use lr_predictions OR rf_predictions OR gbt_predictions\n","pred_df = predictions.select(\"rawPrediction\", \"Exited\").toPandas()\n","\n","# Extract probability of class 1 (churn)\n","pred_df[\"score\"] = pred_df[\"rawPrediction\"].apply(lambda x: float(x[1]))\n","y_true = pred_df[\"Exited\"]\n","y_scores = pred_df[\"score\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X0oirpFev9vM"},"outputs":[],"source":["from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n","roc_auc = auc(fpr, tpr)\n","\n","plt.figure(figsize=(7,6))\n","plt.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.4f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wiCd7ZslBA5K"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Convert Spark predictions to Pandas\n","gbt_pd = gbt_predictions.select(\"rawPrediction\", \"Exited\").toPandas()\n","\n","# Extract positive class probability (GBT gives a vector)\n","gbt_pd[\"score\"] = gbt_pd[\"rawPrediction\"].apply(lambda x: float(x[1]))\n","\n","# True labels and predicted scores\n","y_true = gbt_pd[\"Exited\"]\n","y_scores = gbt_pd[\"score\"]\n","\n","# Compute ROC\n","fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(7,6))\n","plt.plot(fpr, tpr, lw=2, label='GBT ROC (AUC = %0.4f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('GBT ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDJAD42KBIRp"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Convert Spark predictions to Pandas\n","rf_pd = rf_predictions.select(\"rawPrediction\", \"Exited\").toPandas()\n","\n","# Extract probability of class 1 (churn)\n","rf_pd[\"score\"] = rf_pd[\"rawPrediction\"].apply(lambda x: float(x[1]))\n","\n","# True labels and score values\n","y_true = rf_pd[\"Exited\"]\n","y_scores = rf_pd[\"score\"]\n","\n","# Compute ROC curve\n","fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n","roc_auc = auc(fpr, tpr)\n","\n","# Plot ROC\n","plt.figure(figsize=(7,6))\n","plt.plot(fpr, tpr, lw=2, label='Random Forest ROC (AUC = %0.4f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], linestyle='--')\n","\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Random Forest ROC Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}